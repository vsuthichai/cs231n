{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.325071\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** Because the weights are all initialized roughly close to zero, the class scores are also close to zero and roughly equal to one another.  The softmax score is thus 1 / number of classes, and so the cross entropy score is -log(0.1) with 0.1 being (1/10) assuming 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -3.588082 analytic: -3.588083, relative error: 1.692058e-08\n",
      "numerical: 0.778250 analytic: 0.778250, relative error: 1.385312e-08\n",
      "numerical: -1.319771 analytic: -1.319771, relative error: 4.099389e-09\n",
      "numerical: -2.216821 analytic: -2.216822, relative error: 3.156642e-08\n",
      "numerical: -3.290497 analytic: -3.290497, relative error: 1.227587e-08\n",
      "numerical: 1.339214 analytic: 1.339214, relative error: 7.879597e-09\n",
      "numerical: 0.621110 analytic: 0.621110, relative error: 6.574985e-09\n",
      "numerical: -1.758654 analytic: -1.758654, relative error: 1.915224e-08\n",
      "numerical: -1.149479 analytic: -1.149479, relative error: 4.073406e-08\n",
      "numerical: 0.505583 analytic: 0.505583, relative error: 1.326151e-07\n",
      "numerical: 0.289845 analytic: 0.289845, relative error: 1.203221e-08\n",
      "numerical: -0.162994 analytic: -0.162994, relative error: 3.127234e-07\n",
      "numerical: 1.594396 analytic: 1.594396, relative error: 6.887292e-08\n",
      "numerical: 0.838191 analytic: 0.838191, relative error: 4.538127e-08\n",
      "numerical: 0.311618 analytic: 0.311618, relative error: 1.998836e-07\n",
      "numerical: -4.485082 analytic: -4.485082, relative error: 2.423352e-08\n",
      "numerical: -0.550559 analytic: -0.550559, relative error: 1.193845e-09\n",
      "numerical: 1.587867 analytic: 1.587867, relative error: 6.166383e-08\n",
      "numerical: 0.069168 analytic: 0.069168, relative error: 1.049272e-06\n",
      "numerical: -0.665070 analytic: -0.665070, relative error: 8.505542e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.325071e+00 computed in 0.372817s\n",
      "vectorized loss: 2.325071e+00 computed in 0.015882s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 1545.006385\n",
      "iteration 100 / 1000: loss 208.016042\n",
      "iteration 200 / 1000: loss 29.591940\n",
      "iteration 300 / 1000: loss 5.838148\n",
      "iteration 400 / 1000: loss 2.653569\n",
      "iteration 500 / 1000: loss 2.201227\n",
      "iteration 600 / 1000: loss 2.195552\n",
      "iteration 700 / 1000: loss 2.144606\n",
      "iteration 800 / 1000: loss 2.141645\n",
      "iteration 900 / 1000: loss 2.148134\n",
      "iteration 0 / 1000: loss 1445.257200\n",
      "iteration 100 / 1000: loss 214.994850\n",
      "iteration 200 / 1000: loss 33.653413\n",
      "iteration 300 / 1000: loss 6.796959\n",
      "iteration 400 / 1000: loss 2.842094\n",
      "iteration 500 / 1000: loss 2.226478\n",
      "iteration 600 / 1000: loss 2.193152\n",
      "iteration 700 / 1000: loss 2.114191\n",
      "iteration 800 / 1000: loss 2.157778\n",
      "iteration 900 / 1000: loss 2.193575\n",
      "iteration 0 / 1000: loss 1402.459082\n",
      "iteration 100 / 1000: loss 230.812132\n",
      "iteration 200 / 1000: loss 39.444590\n",
      "iteration 300 / 1000: loss 8.270547\n",
      "iteration 400 / 1000: loss 3.136089\n",
      "iteration 500 / 1000: loss 2.300617\n",
      "iteration 600 / 1000: loss 2.159957\n",
      "iteration 700 / 1000: loss 2.140967\n",
      "iteration 800 / 1000: loss 2.115496\n",
      "iteration 900 / 1000: loss 2.147347\n",
      "iteration 0 / 1000: loss 1313.491202\n",
      "iteration 100 / 1000: loss 239.097679\n",
      "iteration 200 / 1000: loss 45.004716\n",
      "iteration 300 / 1000: loss 9.937487\n",
      "iteration 400 / 1000: loss 3.519291\n",
      "iteration 500 / 1000: loss 2.379809\n",
      "iteration 600 / 1000: loss 2.172971\n",
      "iteration 700 / 1000: loss 2.144065\n",
      "iteration 800 / 1000: loss 2.070795\n",
      "iteration 900 / 1000: loss 2.148146\n",
      "iteration 0 / 1000: loss 1216.795865\n",
      "iteration 100 / 1000: loss 244.943865\n",
      "iteration 200 / 1000: loss 50.661794\n",
      "iteration 300 / 1000: loss 11.822335\n",
      "iteration 400 / 1000: loss 4.067843\n",
      "iteration 500 / 1000: loss 2.470440\n",
      "iteration 600 / 1000: loss 2.219982\n",
      "iteration 700 / 1000: loss 2.148096\n",
      "iteration 800 / 1000: loss 2.135241\n",
      "iteration 900 / 1000: loss 2.156231\n",
      "iteration 0 / 1000: loss 1145.026319\n",
      "iteration 100 / 1000: loss 254.811124\n",
      "iteration 200 / 1000: loss 58.010857\n",
      "iteration 300 / 1000: loss 14.497084\n",
      "iteration 400 / 1000: loss 4.851715\n",
      "iteration 500 / 1000: loss 2.700072\n",
      "iteration 600 / 1000: loss 2.275434\n",
      "iteration 700 / 1000: loss 2.133448\n",
      "iteration 800 / 1000: loss 2.114822\n",
      "iteration 900 / 1000: loss 2.131502\n",
      "iteration 0 / 1000: loss 1078.863873\n",
      "iteration 100 / 1000: loss 265.149963\n",
      "iteration 200 / 1000: loss 66.522291\n",
      "iteration 300 / 1000: loss 17.853671\n",
      "iteration 400 / 1000: loss 5.944290\n",
      "iteration 500 / 1000: loss 3.024978\n",
      "iteration 600 / 1000: loss 2.373440\n",
      "iteration 700 / 1000: loss 2.154034\n",
      "iteration 800 / 1000: loss 2.124644\n",
      "iteration 900 / 1000: loss 2.146361\n",
      "iteration 0 / 1000: loss 1001.066949\n",
      "iteration 100 / 1000: loss 272.145950\n",
      "iteration 200 / 1000: loss 75.202413\n",
      "iteration 300 / 1000: loss 21.955163\n",
      "iteration 400 / 1000: loss 7.413990\n",
      "iteration 500 / 1000: loss 3.564085\n",
      "iteration 600 / 1000: loss 2.473979\n",
      "iteration 700 / 1000: loss 2.186745\n",
      "iteration 800 / 1000: loss 2.160768\n",
      "iteration 900 / 1000: loss 2.143332\n",
      "iteration 0 / 1000: loss 925.558933\n",
      "iteration 100 / 1000: loss 278.063940\n",
      "iteration 200 / 1000: loss 84.695302\n",
      "iteration 300 / 1000: loss 26.870997\n",
      "iteration 400 / 1000: loss 9.539691\n",
      "iteration 500 / 1000: loss 4.369559\n",
      "iteration 600 / 1000: loss 2.820838\n",
      "iteration 700 / 1000: loss 2.296478\n",
      "iteration 800 / 1000: loss 2.193840\n",
      "iteration 900 / 1000: loss 2.054579\n",
      "iteration 0 / 1000: loss 859.586954\n",
      "iteration 100 / 1000: loss 285.647613\n",
      "iteration 200 / 1000: loss 95.852541\n",
      "iteration 300 / 1000: loss 33.156276\n",
      "iteration 400 / 1000: loss 12.382385\n",
      "iteration 500 / 1000: loss 5.483718\n",
      "iteration 600 / 1000: loss 3.198088\n",
      "iteration 700 / 1000: loss 2.461007\n",
      "iteration 800 / 1000: loss 2.215987\n",
      "iteration 900 / 1000: loss 2.174448\n",
      "iteration 0 / 1000: loss 1547.936315\n",
      "iteration 100 / 1000: loss 93.855224\n",
      "iteration 200 / 1000: loss 7.572109\n",
      "iteration 300 / 1000: loss 2.462712\n",
      "iteration 400 / 1000: loss 2.184992\n",
      "iteration 500 / 1000: loss 2.144605\n",
      "iteration 600 / 1000: loss 2.129956\n",
      "iteration 700 / 1000: loss 2.129464\n",
      "iteration 800 / 1000: loss 2.174019\n",
      "iteration 900 / 1000: loss 2.136082\n",
      "iteration 0 / 1000: loss 1471.413105\n",
      "iteration 100 / 1000: loss 102.604462\n",
      "iteration 200 / 1000: loss 8.962496\n",
      "iteration 300 / 1000: loss 2.580359\n",
      "iteration 400 / 1000: loss 2.166856\n",
      "iteration 500 / 1000: loss 2.114810\n",
      "iteration 600 / 1000: loss 2.105427\n",
      "iteration 700 / 1000: loss 2.092729\n",
      "iteration 800 / 1000: loss 2.125122\n",
      "iteration 900 / 1000: loss 2.149780\n",
      "iteration 0 / 1000: loss 1391.704530\n",
      "iteration 100 / 1000: loss 111.731266\n",
      "iteration 200 / 1000: loss 10.815860\n",
      "iteration 300 / 1000: loss 2.823834\n",
      "iteration 400 / 1000: loss 2.211077\n",
      "iteration 500 / 1000: loss 2.162906\n",
      "iteration 600 / 1000: loss 2.113322\n",
      "iteration 700 / 1000: loss 2.142313\n",
      "iteration 800 / 1000: loss 2.143784\n",
      "iteration 900 / 1000: loss 2.100040\n",
      "iteration 0 / 1000: loss 1320.323666\n",
      "iteration 100 / 1000: loss 121.856036\n",
      "iteration 200 / 1000: loss 12.975128\n",
      "iteration 300 / 1000: loss 3.172877\n",
      "iteration 400 / 1000: loss 2.267008\n",
      "iteration 500 / 1000: loss 2.133604\n",
      "iteration 600 / 1000: loss 2.095953\n",
      "iteration 700 / 1000: loss 2.142138\n",
      "iteration 800 / 1000: loss 2.132910\n",
      "iteration 900 / 1000: loss 2.130803\n",
      "iteration 0 / 1000: loss 1239.022049\n",
      "iteration 100 / 1000: loss 131.496930\n",
      "iteration 200 / 1000: loss 15.694645\n",
      "iteration 300 / 1000: loss 3.558494\n",
      "iteration 400 / 1000: loss 2.289967\n",
      "iteration 500 / 1000: loss 2.101666\n",
      "iteration 600 / 1000: loss 2.123565\n",
      "iteration 700 / 1000: loss 2.123420\n",
      "iteration 800 / 1000: loss 2.146480\n",
      "iteration 900 / 1000: loss 2.118973\n",
      "iteration 0 / 1000: loss 1122.446280\n",
      "iteration 100 / 1000: loss 137.157446\n",
      "iteration 200 / 1000: loss 18.413769\n",
      "iteration 300 / 1000: loss 4.032330\n",
      "iteration 400 / 1000: loss 2.360956\n",
      "iteration 500 / 1000: loss 2.159439\n",
      "iteration 600 / 1000: loss 2.138465\n",
      "iteration 700 / 1000: loss 2.118071\n",
      "iteration 800 / 1000: loss 2.118011\n",
      "iteration 900 / 1000: loss 2.185339\n",
      "iteration 0 / 1000: loss 1093.180229\n",
      "iteration 100 / 1000: loss 153.526066\n",
      "iteration 200 / 1000: loss 23.161426\n",
      "iteration 300 / 1000: loss 5.051301\n",
      "iteration 400 / 1000: loss 2.549159\n",
      "iteration 500 / 1000: loss 2.174241\n",
      "iteration 600 / 1000: loss 2.120531\n",
      "iteration 700 / 1000: loss 2.059368\n",
      "iteration 800 / 1000: loss 2.141543\n",
      "iteration 900 / 1000: loss 2.096587\n",
      "iteration 0 / 1000: loss 998.162412\n",
      "iteration 100 / 1000: loss 161.114819\n",
      "iteration 200 / 1000: loss 27.538356\n",
      "iteration 300 / 1000: loss 6.173151\n",
      "iteration 400 / 1000: loss 2.770609\n",
      "iteration 500 / 1000: loss 2.179520\n",
      "iteration 600 / 1000: loss 2.057003\n",
      "iteration 700 / 1000: loss 2.100104\n",
      "iteration 800 / 1000: loss 2.151774\n",
      "iteration 900 / 1000: loss 2.095104\n",
      "iteration 0 / 1000: loss 919.328779\n",
      "iteration 100 / 1000: loss 170.898396\n",
      "iteration 200 / 1000: loss 33.204920\n",
      "iteration 300 / 1000: loss 7.822631\n",
      "iteration 400 / 1000: loss 3.138104\n",
      "iteration 500 / 1000: loss 2.271039\n",
      "iteration 600 / 1000: loss 2.115420\n",
      "iteration 700 / 1000: loss 2.070424\n",
      "iteration 800 / 1000: loss 2.189737\n",
      "iteration 900 / 1000: loss 2.091245\n",
      "iteration 0 / 1000: loss 856.849029\n",
      "iteration 100 / 1000: loss 183.463320\n",
      "iteration 200 / 1000: loss 40.536394\n",
      "iteration 300 / 1000: loss 10.259974\n",
      "iteration 400 / 1000: loss 3.823482\n",
      "iteration 500 / 1000: loss 2.494157\n",
      "iteration 600 / 1000: loss 2.134283\n",
      "iteration 700 / 1000: loss 2.094392\n",
      "iteration 800 / 1000: loss 2.074293\n",
      "iteration 900 / 1000: loss 2.123557\n",
      "iteration 0 / 1000: loss 1524.312379\n",
      "iteration 100 / 1000: loss 42.119570\n",
      "iteration 200 / 1000: loss 3.190060\n",
      "iteration 300 / 1000: loss 2.132565\n",
      "iteration 400 / 1000: loss 2.133102\n",
      "iteration 500 / 1000: loss 2.164954\n",
      "iteration 600 / 1000: loss 2.082107\n",
      "iteration 700 / 1000: loss 2.135480\n",
      "iteration 800 / 1000: loss 2.118878\n",
      "iteration 900 / 1000: loss 2.125473\n",
      "iteration 0 / 1000: loss 1478.274250\n",
      "iteration 100 / 1000: loss 48.653464\n",
      "iteration 200 / 1000: loss 3.589140\n",
      "iteration 300 / 1000: loss 2.166054\n",
      "iteration 400 / 1000: loss 2.158889\n",
      "iteration 500 / 1000: loss 2.107679\n",
      "iteration 600 / 1000: loss 2.083671\n",
      "iteration 700 / 1000: loss 2.115960\n",
      "iteration 800 / 1000: loss 2.133054\n",
      "iteration 900 / 1000: loss 2.096789\n",
      "iteration 0 / 1000: loss 1386.140530\n",
      "iteration 100 / 1000: loss 54.561831\n",
      "iteration 200 / 1000: loss 4.125483\n",
      "iteration 300 / 1000: loss 2.208089\n",
      "iteration 400 / 1000: loss 2.136703\n",
      "iteration 500 / 1000: loss 2.109424\n",
      "iteration 600 / 1000: loss 2.103667\n",
      "iteration 700 / 1000: loss 2.174226\n",
      "iteration 800 / 1000: loss 2.120126\n",
      "iteration 900 / 1000: loss 2.149045\n",
      "iteration 0 / 1000: loss 1304.394258\n",
      "iteration 100 / 1000: loss 61.309480\n",
      "iteration 200 / 1000: loss 4.863224\n",
      "iteration 300 / 1000: loss 2.271507\n",
      "iteration 400 / 1000: loss 2.140586\n",
      "iteration 500 / 1000: loss 2.140780\n",
      "iteration 600 / 1000: loss 2.158969\n",
      "iteration 700 / 1000: loss 2.082523\n",
      "iteration 800 / 1000: loss 2.074815\n",
      "iteration 900 / 1000: loss 2.066083\n",
      "iteration 0 / 1000: loss 1235.566381\n",
      "iteration 100 / 1000: loss 69.448114\n",
      "iteration 200 / 1000: loss 5.841763\n",
      "iteration 300 / 1000: loss 2.334815\n",
      "iteration 400 / 1000: loss 2.133282\n",
      "iteration 500 / 1000: loss 2.082192\n",
      "iteration 600 / 1000: loss 2.108930\n",
      "iteration 700 / 1000: loss 2.154629\n",
      "iteration 800 / 1000: loss 2.064475\n",
      "iteration 900 / 1000: loss 2.093110\n",
      "iteration 0 / 1000: loss 1159.461207\n",
      "iteration 100 / 1000: loss 78.024398\n",
      "iteration 200 / 1000: loss 7.100317\n",
      "iteration 300 / 1000: loss 2.476449\n",
      "iteration 400 / 1000: loss 2.093097\n",
      "iteration 500 / 1000: loss 2.127858\n",
      "iteration 600 / 1000: loss 2.143221\n",
      "iteration 700 / 1000: loss 2.128457\n",
      "iteration 800 / 1000: loss 2.163120\n",
      "iteration 900 / 1000: loss 2.075348\n",
      "iteration 0 / 1000: loss 1079.598549\n",
      "iteration 100 / 1000: loss 86.978287\n",
      "iteration 200 / 1000: loss 8.767330\n",
      "iteration 300 / 1000: loss 2.658472\n",
      "iteration 400 / 1000: loss 2.186886\n",
      "iteration 500 / 1000: loss 2.132199\n",
      "iteration 600 / 1000: loss 2.080486\n",
      "iteration 700 / 1000: loss 2.112374\n",
      "iteration 800 / 1000: loss 2.170480\n",
      "iteration 900 / 1000: loss 2.163845\n",
      "iteration 0 / 1000: loss 993.242374\n",
      "iteration 100 / 1000: loss 95.784766\n",
      "iteration 200 / 1000: loss 10.967974\n",
      "iteration 300 / 1000: loss 2.947049\n",
      "iteration 400 / 1000: loss 2.130524\n",
      "iteration 500 / 1000: loss 2.115999\n",
      "iteration 600 / 1000: loss 2.128111\n",
      "iteration 700 / 1000: loss 2.145553\n",
      "iteration 800 / 1000: loss 2.106149\n",
      "iteration 900 / 1000: loss 2.124978\n",
      "iteration 0 / 1000: loss 945.344948\n",
      "iteration 100 / 1000: loss 108.966969\n",
      "iteration 200 / 1000: loss 14.191613\n",
      "iteration 300 / 1000: loss 3.513965\n",
      "iteration 400 / 1000: loss 2.249614\n",
      "iteration 500 / 1000: loss 2.147451\n",
      "iteration 600 / 1000: loss 2.049328\n",
      "iteration 700 / 1000: loss 2.070801\n",
      "iteration 800 / 1000: loss 2.069095\n",
      "iteration 900 / 1000: loss 2.089634\n",
      "iteration 0 / 1000: loss 855.794706\n",
      "iteration 100 / 1000: loss 118.132642\n",
      "iteration 200 / 1000: loss 17.867608\n",
      "iteration 300 / 1000: loss 4.257954\n",
      "iteration 400 / 1000: loss 2.394306\n",
      "iteration 500 / 1000: loss 2.123197\n",
      "iteration 600 / 1000: loss 2.039881\n",
      "iteration 700 / 1000: loss 2.117150\n",
      "iteration 800 / 1000: loss 2.121870\n",
      "iteration 900 / 1000: loss 2.105183\n",
      "iteration 0 / 1000: loss 1554.602780\n",
      "iteration 100 / 1000: loss 20.163017\n",
      "iteration 200 / 1000: loss 2.377361\n",
      "iteration 300 / 1000: loss 2.155507\n",
      "iteration 400 / 1000: loss 2.115901\n",
      "iteration 500 / 1000: loss 2.149518\n",
      "iteration 600 / 1000: loss 2.145768\n",
      "iteration 700 / 1000: loss 2.141016\n",
      "iteration 800 / 1000: loss 2.166754\n",
      "iteration 900 / 1000: loss 2.129394\n",
      "iteration 0 / 1000: loss 1469.917232\n",
      "iteration 100 / 1000: loss 23.501190\n",
      "iteration 200 / 1000: loss 2.462364\n",
      "iteration 300 / 1000: loss 2.151435\n",
      "iteration 400 / 1000: loss 2.093852\n",
      "iteration 500 / 1000: loss 2.122511\n",
      "iteration 600 / 1000: loss 2.142897\n",
      "iteration 700 / 1000: loss 2.157555\n",
      "iteration 800 / 1000: loss 2.164036\n",
      "iteration 900 / 1000: loss 2.179218\n",
      "iteration 0 / 1000: loss 1395.951804\n",
      "iteration 100 / 1000: loss 27.507370\n",
      "iteration 200 / 1000: loss 2.518264\n",
      "iteration 300 / 1000: loss 2.150265\n",
      "iteration 400 / 1000: loss 2.115836\n",
      "iteration 500 / 1000: loss 2.163707\n",
      "iteration 600 / 1000: loss 2.124675\n",
      "iteration 700 / 1000: loss 2.156466\n",
      "iteration 800 / 1000: loss 2.146010\n",
      "iteration 900 / 1000: loss 2.075538\n",
      "iteration 0 / 1000: loss 1322.498032\n",
      "iteration 100 / 1000: loss 32.173490\n",
      "iteration 200 / 1000: loss 2.849211\n",
      "iteration 300 / 1000: loss 2.113576\n",
      "iteration 400 / 1000: loss 2.173190\n",
      "iteration 500 / 1000: loss 2.136691\n",
      "iteration 600 / 1000: loss 2.127020\n",
      "iteration 700 / 1000: loss 2.128367\n",
      "iteration 800 / 1000: loss 2.141984\n",
      "iteration 900 / 1000: loss 2.153473\n",
      "iteration 0 / 1000: loss 1233.830314\n",
      "iteration 100 / 1000: loss 37.226766\n",
      "iteration 200 / 1000: loss 3.105167\n",
      "iteration 300 / 1000: loss 2.153513\n",
      "iteration 400 / 1000: loss 2.117194\n",
      "iteration 500 / 1000: loss 2.120616\n",
      "iteration 600 / 1000: loss 2.155364\n",
      "iteration 700 / 1000: loss 2.135074\n",
      "iteration 800 / 1000: loss 2.144887\n",
      "iteration 900 / 1000: loss 2.149998\n",
      "iteration 0 / 1000: loss 1159.088991\n",
      "iteration 100 / 1000: loss 43.242772\n",
      "iteration 200 / 1000: loss 3.596005\n",
      "iteration 300 / 1000: loss 2.164855\n",
      "iteration 400 / 1000: loss 2.139195\n",
      "iteration 500 / 1000: loss 2.123980\n",
      "iteration 600 / 1000: loss 2.110234\n",
      "iteration 700 / 1000: loss 2.144224\n",
      "iteration 800 / 1000: loss 2.119221\n",
      "iteration 900 / 1000: loss 2.162828\n",
      "iteration 0 / 1000: loss 1091.900724\n",
      "iteration 100 / 1000: loss 50.623596\n",
      "iteration 200 / 1000: loss 4.287734\n",
      "iteration 300 / 1000: loss 2.196898\n",
      "iteration 400 / 1000: loss 2.095485\n",
      "iteration 500 / 1000: loss 2.125893\n",
      "iteration 600 / 1000: loss 2.090059\n",
      "iteration 700 / 1000: loss 2.091632\n",
      "iteration 800 / 1000: loss 2.111507\n",
      "iteration 900 / 1000: loss 2.099631\n",
      "iteration 0 / 1000: loss 997.357977\n",
      "iteration 100 / 1000: loss 57.609761\n",
      "iteration 200 / 1000: loss 5.202756\n",
      "iteration 300 / 1000: loss 2.336871\n",
      "iteration 400 / 1000: loss 2.124304\n",
      "iteration 500 / 1000: loss 2.104148\n",
      "iteration 600 / 1000: loss 2.074454\n",
      "iteration 700 / 1000: loss 2.101499\n",
      "iteration 800 / 1000: loss 2.151009\n",
      "iteration 900 / 1000: loss 2.137651\n",
      "iteration 0 / 1000: loss 931.889107\n",
      "iteration 100 / 1000: loss 66.829973\n",
      "iteration 200 / 1000: loss 6.613176\n",
      "iteration 300 / 1000: loss 2.455862\n",
      "iteration 400 / 1000: loss 2.102730\n",
      "iteration 500 / 1000: loss 2.110779\n",
      "iteration 600 / 1000: loss 2.087333\n",
      "iteration 700 / 1000: loss 2.149583\n",
      "iteration 800 / 1000: loss 2.080173\n",
      "iteration 900 / 1000: loss 2.085985\n",
      "iteration 0 / 1000: loss 861.548242\n",
      "iteration 100 / 1000: loss 76.772901\n",
      "iteration 200 / 1000: loss 8.673525\n",
      "iteration 300 / 1000: loss 2.669971\n",
      "iteration 400 / 1000: loss 2.159925\n",
      "iteration 500 / 1000: loss 2.066743\n",
      "iteration 600 / 1000: loss 2.098239\n",
      "iteration 700 / 1000: loss 2.107552\n",
      "iteration 800 / 1000: loss 2.069811\n",
      "iteration 900 / 1000: loss 2.053734\n",
      "iteration 0 / 1000: loss 1549.919130\n",
      "iteration 100 / 1000: loss 10.054321\n",
      "iteration 200 / 1000: loss 2.154220\n",
      "iteration 300 / 1000: loss 2.185321\n",
      "iteration 400 / 1000: loss 2.145797\n",
      "iteration 500 / 1000: loss 2.103015\n",
      "iteration 600 / 1000: loss 2.137796\n",
      "iteration 700 / 1000: loss 2.140608\n",
      "iteration 800 / 1000: loss 2.128896\n",
      "iteration 900 / 1000: loss 2.147792\n",
      "iteration 0 / 1000: loss 1441.559161\n",
      "iteration 100 / 1000: loss 11.723669\n",
      "iteration 200 / 1000: loss 2.184061\n",
      "iteration 300 / 1000: loss 2.153691\n",
      "iteration 400 / 1000: loss 2.126390\n",
      "iteration 500 / 1000: loss 2.154903\n",
      "iteration 600 / 1000: loss 2.161185\n",
      "iteration 700 / 1000: loss 2.121139\n",
      "iteration 800 / 1000: loss 2.153842\n",
      "iteration 900 / 1000: loss 2.121826\n",
      "iteration 0 / 1000: loss 1396.609935\n",
      "iteration 100 / 1000: loss 14.249154\n",
      "iteration 200 / 1000: loss 2.247658\n",
      "iteration 300 / 1000: loss 2.127060\n",
      "iteration 400 / 1000: loss 2.126069\n",
      "iteration 500 / 1000: loss 2.164532\n",
      "iteration 600 / 1000: loss 2.094377\n",
      "iteration 700 / 1000: loss 2.163082\n",
      "iteration 800 / 1000: loss 2.101385\n",
      "iteration 900 / 1000: loss 2.147838\n",
      "iteration 0 / 1000: loss 1315.271614\n",
      "iteration 100 / 1000: loss 17.015634\n",
      "iteration 200 / 1000: loss 2.294184\n",
      "iteration 300 / 1000: loss 2.164132\n",
      "iteration 400 / 1000: loss 2.142761\n",
      "iteration 500 / 1000: loss 2.103086\n",
      "iteration 600 / 1000: loss 2.132684\n",
      "iteration 700 / 1000: loss 2.117347\n",
      "iteration 800 / 1000: loss 2.085785\n",
      "iteration 900 / 1000: loss 2.088902\n",
      "iteration 0 / 1000: loss 1238.494589\n",
      "iteration 100 / 1000: loss 20.442093\n",
      "iteration 200 / 1000: loss 2.420128\n",
      "iteration 300 / 1000: loss 2.075596\n",
      "iteration 400 / 1000: loss 2.131096\n",
      "iteration 500 / 1000: loss 2.106373\n",
      "iteration 600 / 1000: loss 2.128928\n",
      "iteration 700 / 1000: loss 2.146304\n",
      "iteration 800 / 1000: loss 2.155429\n",
      "iteration 900 / 1000: loss 2.102494\n",
      "iteration 0 / 1000: loss 1159.337999\n",
      "iteration 100 / 1000: loss 24.469996\n",
      "iteration 200 / 1000: loss 2.586046\n",
      "iteration 300 / 1000: loss 2.197787\n",
      "iteration 400 / 1000: loss 2.105427\n",
      "iteration 500 / 1000: loss 2.089682\n",
      "iteration 600 / 1000: loss 2.127682\n",
      "iteration 700 / 1000: loss 2.131865\n",
      "iteration 800 / 1000: loss 2.142316\n",
      "iteration 900 / 1000: loss 2.165207\n",
      "iteration 0 / 1000: loss 1081.663171\n",
      "iteration 100 / 1000: loss 29.282289\n",
      "iteration 200 / 1000: loss 2.791926\n",
      "iteration 300 / 1000: loss 2.111094\n",
      "iteration 400 / 1000: loss 2.179739\n",
      "iteration 500 / 1000: loss 2.089724\n",
      "iteration 600 / 1000: loss 2.043488\n",
      "iteration 700 / 1000: loss 2.052716\n",
      "iteration 800 / 1000: loss 2.146720\n",
      "iteration 900 / 1000: loss 2.137792\n",
      "iteration 0 / 1000: loss 994.443935\n",
      "iteration 100 / 1000: loss 34.636683\n",
      "iteration 200 / 1000: loss 3.198301\n",
      "iteration 300 / 1000: loss 2.166949\n",
      "iteration 400 / 1000: loss 2.164454\n",
      "iteration 500 / 1000: loss 2.113215\n",
      "iteration 600 / 1000: loss 2.074457\n",
      "iteration 700 / 1000: loss 2.168598\n",
      "iteration 800 / 1000: loss 2.160487\n",
      "iteration 900 / 1000: loss 2.139846\n",
      "iteration 0 / 1000: loss 930.073092\n",
      "iteration 100 / 1000: loss 41.649808\n",
      "iteration 200 / 1000: loss 3.803091\n",
      "iteration 300 / 1000: loss 2.232901\n",
      "iteration 400 / 1000: loss 2.141690\n",
      "iteration 500 / 1000: loss 2.068126\n",
      "iteration 600 / 1000: loss 2.076606\n",
      "iteration 700 / 1000: loss 2.146216\n",
      "iteration 800 / 1000: loss 2.099678\n",
      "iteration 900 / 1000: loss 2.092934\n",
      "iteration 0 / 1000: loss 867.402325\n",
      "iteration 100 / 1000: loss 50.220610\n",
      "iteration 200 / 1000: loss 4.783544\n",
      "iteration 300 / 1000: loss 2.195844\n",
      "iteration 400 / 1000: loss 2.045162\n",
      "iteration 500 / 1000: loss 2.093128\n",
      "iteration 600 / 1000: loss 2.059096\n",
      "iteration 700 / 1000: loss 2.055797\n",
      "iteration 800 / 1000: loss 2.062868\n",
      "iteration 900 / 1000: loss 2.107314\n",
      "iteration 0 / 1000: loss 1561.526438\n",
      "iteration 100 / 1000: loss 5.630227\n",
      "iteration 200 / 1000: loss 2.171556\n",
      "iteration 300 / 1000: loss 2.166790\n",
      "iteration 400 / 1000: loss 2.121278\n",
      "iteration 500 / 1000: loss 2.081084\n",
      "iteration 600 / 1000: loss 2.185466\n",
      "iteration 700 / 1000: loss 2.078103\n",
      "iteration 800 / 1000: loss 2.128157\n",
      "iteration 900 / 1000: loss 2.122084\n",
      "iteration 0 / 1000: loss 1464.081500\n",
      "iteration 100 / 1000: loss 6.581005\n",
      "iteration 200 / 1000: loss 2.123318\n",
      "iteration 300 / 1000: loss 2.129278\n",
      "iteration 400 / 1000: loss 2.112842\n",
      "iteration 500 / 1000: loss 2.143170\n",
      "iteration 600 / 1000: loss 2.154946\n",
      "iteration 700 / 1000: loss 2.155791\n",
      "iteration 800 / 1000: loss 2.175797\n",
      "iteration 900 / 1000: loss 2.148585\n",
      "iteration 0 / 1000: loss 1358.422823\n",
      "iteration 100 / 1000: loss 7.779915\n",
      "iteration 200 / 1000: loss 2.183926\n",
      "iteration 300 / 1000: loss 2.128067\n",
      "iteration 400 / 1000: loss 2.122126\n",
      "iteration 500 / 1000: loss 2.130617\n",
      "iteration 600 / 1000: loss 2.136366\n",
      "iteration 700 / 1000: loss 2.211384\n",
      "iteration 800 / 1000: loss 2.177490\n",
      "iteration 900 / 1000: loss 2.139476\n",
      "iteration 0 / 1000: loss 1321.184005\n",
      "iteration 100 / 1000: loss 9.653189\n",
      "iteration 200 / 1000: loss 2.146631\n",
      "iteration 300 / 1000: loss 2.118084\n",
      "iteration 400 / 1000: loss 2.132927\n",
      "iteration 500 / 1000: loss 2.135231\n",
      "iteration 600 / 1000: loss 2.119152\n",
      "iteration 700 / 1000: loss 2.149437\n",
      "iteration 800 / 1000: loss 2.124142\n",
      "iteration 900 / 1000: loss 2.141380\n",
      "iteration 0 / 1000: loss 1207.543949\n",
      "iteration 100 / 1000: loss 11.394987\n",
      "iteration 200 / 1000: loss 2.246183\n",
      "iteration 300 / 1000: loss 2.109530\n",
      "iteration 400 / 1000: loss 2.160207\n",
      "iteration 500 / 1000: loss 2.129521\n",
      "iteration 600 / 1000: loss 2.132597\n",
      "iteration 700 / 1000: loss 2.104484\n",
      "iteration 800 / 1000: loss 2.143078\n",
      "iteration 900 / 1000: loss 2.153840\n",
      "iteration 0 / 1000: loss 1171.046831\n",
      "iteration 100 / 1000: loss 14.307387\n",
      "iteration 200 / 1000: loss 2.253501\n",
      "iteration 300 / 1000: loss 2.151984\n",
      "iteration 400 / 1000: loss 2.138515\n",
      "iteration 500 / 1000: loss 2.155566\n",
      "iteration 600 / 1000: loss 2.105408\n",
      "iteration 700 / 1000: loss 2.140035\n",
      "iteration 800 / 1000: loss 2.064258\n",
      "iteration 900 / 1000: loss 2.128057\n",
      "iteration 0 / 1000: loss 1083.794318\n",
      "iteration 100 / 1000: loss 17.483599\n",
      "iteration 200 / 1000: loss 2.352490\n",
      "iteration 300 / 1000: loss 2.108095\n",
      "iteration 400 / 1000: loss 2.124185\n",
      "iteration 500 / 1000: loss 2.100097\n",
      "iteration 600 / 1000: loss 2.099244\n",
      "iteration 700 / 1000: loss 2.126122\n",
      "iteration 800 / 1000: loss 2.157903\n",
      "iteration 900 / 1000: loss 2.089360\n",
      "iteration 0 / 1000: loss 999.163397\n",
      "iteration 100 / 1000: loss 21.350958\n",
      "iteration 200 / 1000: loss 2.497991\n",
      "iteration 300 / 1000: loss 2.094757\n",
      "iteration 400 / 1000: loss 2.149728\n",
      "iteration 500 / 1000: loss 2.085991\n",
      "iteration 600 / 1000: loss 2.128028\n",
      "iteration 700 / 1000: loss 2.089739\n",
      "iteration 800 / 1000: loss 2.145950\n",
      "iteration 900 / 1000: loss 2.020965\n",
      "iteration 0 / 1000: loss 936.979149\n",
      "iteration 100 / 1000: loss 26.572222\n",
      "iteration 200 / 1000: loss 2.725509\n",
      "iteration 300 / 1000: loss 2.125604\n",
      "iteration 400 / 1000: loss 2.099860\n",
      "iteration 500 / 1000: loss 2.073816\n",
      "iteration 600 / 1000: loss 2.118680\n",
      "iteration 700 / 1000: loss 2.134020\n",
      "iteration 800 / 1000: loss 2.121273\n",
      "iteration 900 / 1000: loss 2.116288\n",
      "iteration 0 / 1000: loss 850.213515\n",
      "iteration 100 / 1000: loss 32.208983\n",
      "iteration 200 / 1000: loss 3.116435\n",
      "iteration 300 / 1000: loss 2.190233\n",
      "iteration 400 / 1000: loss 2.099693\n",
      "iteration 500 / 1000: loss 2.093130\n",
      "iteration 600 / 1000: loss 2.119999\n",
      "iteration 700 / 1000: loss 2.076128\n",
      "iteration 800 / 1000: loss 2.063205\n",
      "iteration 900 / 1000: loss 2.112023\n",
      "iteration 0 / 1000: loss 1548.517399\n",
      "iteration 100 / 1000: loss 3.655493\n",
      "iteration 200 / 1000: loss 2.164251\n",
      "iteration 300 / 1000: loss 2.120985\n",
      "iteration 400 / 1000: loss 2.182363\n",
      "iteration 500 / 1000: loss 2.114580\n",
      "iteration 600 / 1000: loss 2.111325\n",
      "iteration 700 / 1000: loss 2.197153\n",
      "iteration 800 / 1000: loss 2.168062\n",
      "iteration 900 / 1000: loss 2.156958\n",
      "iteration 0 / 1000: loss 1478.907854\n",
      "iteration 100 / 1000: loss 4.281723\n",
      "iteration 200 / 1000: loss 2.132111\n",
      "iteration 300 / 1000: loss 2.111373\n",
      "iteration 400 / 1000: loss 2.143148\n",
      "iteration 500 / 1000: loss 2.167937\n",
      "iteration 600 / 1000: loss 2.145744\n",
      "iteration 700 / 1000: loss 2.144059\n",
      "iteration 800 / 1000: loss 2.120214\n",
      "iteration 900 / 1000: loss 2.131753\n",
      "iteration 0 / 1000: loss 1404.446452\n",
      "iteration 100 / 1000: loss 4.941873\n",
      "iteration 200 / 1000: loss 2.137629\n",
      "iteration 300 / 1000: loss 2.161706\n",
      "iteration 400 / 1000: loss 2.102705\n",
      "iteration 500 / 1000: loss 2.075948\n",
      "iteration 600 / 1000: loss 2.085725\n",
      "iteration 700 / 1000: loss 2.136937\n",
      "iteration 800 / 1000: loss 2.148491\n",
      "iteration 900 / 1000: loss 2.130602\n",
      "iteration 0 / 1000: loss 1294.086097\n",
      "iteration 100 / 1000: loss 5.777451\n",
      "iteration 200 / 1000: loss 2.133799\n",
      "iteration 300 / 1000: loss 2.126143\n",
      "iteration 400 / 1000: loss 2.139471\n",
      "iteration 500 / 1000: loss 2.137323\n",
      "iteration 600 / 1000: loss 2.120506\n",
      "iteration 700 / 1000: loss 2.151334\n",
      "iteration 800 / 1000: loss 2.188447\n",
      "iteration 900 / 1000: loss 2.117501\n",
      "iteration 0 / 1000: loss 1243.986912\n",
      "iteration 100 / 1000: loss 7.031740\n",
      "iteration 200 / 1000: loss 2.154398\n",
      "iteration 300 / 1000: loss 2.097370\n",
      "iteration 400 / 1000: loss 2.133741\n",
      "iteration 500 / 1000: loss 2.150929\n",
      "iteration 600 / 1000: loss 2.078938\n",
      "iteration 700 / 1000: loss 2.196276\n",
      "iteration 800 / 1000: loss 2.114264\n",
      "iteration 900 / 1000: loss 2.117976\n",
      "iteration 0 / 1000: loss 1165.915126\n",
      "iteration 100 / 1000: loss 8.702130\n",
      "iteration 200 / 1000: loss 2.160340\n",
      "iteration 300 / 1000: loss 2.122744\n",
      "iteration 400 / 1000: loss 2.093130\n",
      "iteration 500 / 1000: loss 2.136484\n",
      "iteration 600 / 1000: loss 2.168603\n",
      "iteration 700 / 1000: loss 2.099673\n",
      "iteration 800 / 1000: loss 2.153588\n",
      "iteration 900 / 1000: loss 2.138075\n",
      "iteration 0 / 1000: loss 1084.961137\n",
      "iteration 100 / 1000: loss 10.782633\n",
      "iteration 200 / 1000: loss 2.179616\n",
      "iteration 300 / 1000: loss 2.094966\n",
      "iteration 400 / 1000: loss 2.125020\n",
      "iteration 500 / 1000: loss 2.075033\n",
      "iteration 600 / 1000: loss 2.133418\n",
      "iteration 700 / 1000: loss 2.136329\n",
      "iteration 800 / 1000: loss 2.074299\n",
      "iteration 900 / 1000: loss 2.182494\n",
      "iteration 0 / 1000: loss 1004.226100\n",
      "iteration 100 / 1000: loss 13.455203\n",
      "iteration 200 / 1000: loss 2.212567\n",
      "iteration 300 / 1000: loss 2.046266\n",
      "iteration 400 / 1000: loss 2.017395\n",
      "iteration 500 / 1000: loss 2.112552\n",
      "iteration 600 / 1000: loss 2.068257\n",
      "iteration 700 / 1000: loss 2.099806\n",
      "iteration 800 / 1000: loss 2.142094\n",
      "iteration 900 / 1000: loss 2.062755\n",
      "iteration 0 / 1000: loss 921.709277\n",
      "iteration 100 / 1000: loss 16.886508\n",
      "iteration 200 / 1000: loss 2.369227\n",
      "iteration 300 / 1000: loss 2.094963\n",
      "iteration 400 / 1000: loss 2.108801\n",
      "iteration 500 / 1000: loss 2.104229\n",
      "iteration 600 / 1000: loss 2.084741\n",
      "iteration 700 / 1000: loss 2.173903\n",
      "iteration 800 / 1000: loss 2.088787\n",
      "iteration 900 / 1000: loss 2.070116\n",
      "iteration 0 / 1000: loss 847.128413\n",
      "iteration 100 / 1000: loss 21.307574\n",
      "iteration 200 / 1000: loss 2.498094\n",
      "iteration 300 / 1000: loss 2.068822\n",
      "iteration 400 / 1000: loss 2.074841\n",
      "iteration 500 / 1000: loss 2.109888\n",
      "iteration 600 / 1000: loss 2.093691\n",
      "iteration 700 / 1000: loss 2.132947\n",
      "iteration 800 / 1000: loss 2.076707\n",
      "iteration 900 / 1000: loss 2.047147\n",
      "iteration 0 / 1000: loss 1536.090629\n",
      "iteration 100 / 1000: loss 2.850900\n",
      "iteration 200 / 1000: loss 2.108823\n",
      "iteration 300 / 1000: loss 2.111826\n",
      "iteration 400 / 1000: loss 2.151554\n",
      "iteration 500 / 1000: loss 2.162383\n",
      "iteration 600 / 1000: loss 2.142706\n",
      "iteration 700 / 1000: loss 2.122965\n",
      "iteration 800 / 1000: loss 2.164784\n",
      "iteration 900 / 1000: loss 2.140908\n",
      "iteration 0 / 1000: loss 1483.415793\n",
      "iteration 100 / 1000: loss 3.015718\n",
      "iteration 200 / 1000: loss 2.152077\n",
      "iteration 300 / 1000: loss 2.119939\n",
      "iteration 400 / 1000: loss 2.137291\n",
      "iteration 500 / 1000: loss 2.213553\n",
      "iteration 600 / 1000: loss 2.116290\n",
      "iteration 700 / 1000: loss 2.149557\n",
      "iteration 800 / 1000: loss 2.107260\n",
      "iteration 900 / 1000: loss 2.168435\n",
      "iteration 0 / 1000: loss 1393.767374\n",
      "iteration 100 / 1000: loss 3.452959\n",
      "iteration 200 / 1000: loss 2.158185\n",
      "iteration 300 / 1000: loss 2.132141\n",
      "iteration 400 / 1000: loss 2.147807\n",
      "iteration 500 / 1000: loss 2.135784\n",
      "iteration 600 / 1000: loss 2.131779\n",
      "iteration 700 / 1000: loss 2.158182\n",
      "iteration 800 / 1000: loss 2.115209\n",
      "iteration 900 / 1000: loss 2.147836\n",
      "iteration 0 / 1000: loss 1303.028381\n",
      "iteration 100 / 1000: loss 3.943850\n",
      "iteration 200 / 1000: loss 2.113052\n",
      "iteration 300 / 1000: loss 2.112328\n",
      "iteration 400 / 1000: loss 2.131210\n",
      "iteration 500 / 1000: loss 2.115499\n",
      "iteration 600 / 1000: loss 2.133067\n",
      "iteration 700 / 1000: loss 2.168504\n",
      "iteration 800 / 1000: loss 2.112293\n",
      "iteration 900 / 1000: loss 2.137806\n",
      "iteration 0 / 1000: loss 1238.493071\n",
      "iteration 100 / 1000: loss 4.616023\n",
      "iteration 200 / 1000: loss 2.116648\n",
      "iteration 300 / 1000: loss 2.114084\n",
      "iteration 400 / 1000: loss 2.090996\n",
      "iteration 500 / 1000: loss 2.103138\n",
      "iteration 600 / 1000: loss 2.161027\n",
      "iteration 700 / 1000: loss 2.185943\n",
      "iteration 800 / 1000: loss 2.130019\n",
      "iteration 900 / 1000: loss 2.139488\n",
      "iteration 0 / 1000: loss 1168.947941\n",
      "iteration 100 / 1000: loss 5.623347\n",
      "iteration 200 / 1000: loss 2.112988\n",
      "iteration 300 / 1000: loss 2.202717\n",
      "iteration 400 / 1000: loss 2.146836\n",
      "iteration 500 / 1000: loss 2.168099\n",
      "iteration 600 / 1000: loss 2.150590\n",
      "iteration 700 / 1000: loss 2.130423\n",
      "iteration 800 / 1000: loss 2.081495\n",
      "iteration 900 / 1000: loss 2.189978\n",
      "iteration 0 / 1000: loss 1076.569892\n",
      "iteration 100 / 1000: loss 6.921908\n",
      "iteration 200 / 1000: loss 2.122395\n",
      "iteration 300 / 1000: loss 2.114116\n",
      "iteration 400 / 1000: loss 2.120882\n",
      "iteration 500 / 1000: loss 2.107922\n",
      "iteration 600 / 1000: loss 2.066374\n",
      "iteration 700 / 1000: loss 2.195754\n",
      "iteration 800 / 1000: loss 2.077402\n",
      "iteration 900 / 1000: loss 2.099795\n",
      "iteration 0 / 1000: loss 1016.221191\n",
      "iteration 100 / 1000: loss 8.831679\n",
      "iteration 200 / 1000: loss 2.143328\n",
      "iteration 300 / 1000: loss 2.138721\n",
      "iteration 400 / 1000: loss 2.119428\n",
      "iteration 500 / 1000: loss 2.046123\n",
      "iteration 600 / 1000: loss 2.174064\n",
      "iteration 700 / 1000: loss 2.030480\n",
      "iteration 800 / 1000: loss 2.115277\n",
      "iteration 900 / 1000: loss 2.109450\n",
      "iteration 0 / 1000: loss 922.084623\n",
      "iteration 100 / 1000: loss 11.169122\n",
      "iteration 200 / 1000: loss 2.178247\n",
      "iteration 300 / 1000: loss 2.142411\n",
      "iteration 400 / 1000: loss 2.126714\n",
      "iteration 500 / 1000: loss 2.031996\n",
      "iteration 600 / 1000: loss 2.124631\n",
      "iteration 700 / 1000: loss 2.079058\n",
      "iteration 800 / 1000: loss 2.096550\n",
      "iteration 900 / 1000: loss 2.123770\n",
      "iteration 0 / 1000: loss 854.018293\n",
      "iteration 100 / 1000: loss 14.413852\n",
      "iteration 200 / 1000: loss 2.263403\n",
      "iteration 300 / 1000: loss 2.117745\n",
      "iteration 400 / 1000: loss 2.104379\n",
      "iteration 500 / 1000: loss 2.091472\n",
      "iteration 600 / 1000: loss 2.172727\n",
      "iteration 700 / 1000: loss 2.088608\n",
      "iteration 800 / 1000: loss 2.093486\n",
      "iteration 900 / 1000: loss 2.053135\n",
      "iteration 0 / 1000: loss 1541.229963\n",
      "iteration 100 / 1000: loss 2.397662\n",
      "iteration 200 / 1000: loss 2.117803\n",
      "iteration 300 / 1000: loss 2.187080\n",
      "iteration 400 / 1000: loss 2.124042\n",
      "iteration 500 / 1000: loss 2.133060\n",
      "iteration 600 / 1000: loss 2.154038\n",
      "iteration 700 / 1000: loss 2.184498\n",
      "iteration 800 / 1000: loss 2.137160\n",
      "iteration 900 / 1000: loss 2.122947\n",
      "iteration 0 / 1000: loss 1460.927958\n",
      "iteration 100 / 1000: loss 2.564735\n",
      "iteration 200 / 1000: loss 2.154358\n",
      "iteration 300 / 1000: loss 2.146451\n",
      "iteration 400 / 1000: loss 2.137972\n",
      "iteration 500 / 1000: loss 2.133301\n",
      "iteration 600 / 1000: loss 2.104257\n",
      "iteration 700 / 1000: loss 2.138316\n",
      "iteration 800 / 1000: loss 2.180927\n",
      "iteration 900 / 1000: loss 2.171317\n",
      "iteration 0 / 1000: loss 1396.764684\n",
      "iteration 100 / 1000: loss 2.784102\n",
      "iteration 200 / 1000: loss 2.140278\n",
      "iteration 300 / 1000: loss 2.180630\n",
      "iteration 400 / 1000: loss 2.151124\n",
      "iteration 500 / 1000: loss 2.152833\n",
      "iteration 600 / 1000: loss 2.143162\n",
      "iteration 700 / 1000: loss 2.096198\n",
      "iteration 800 / 1000: loss 2.113591\n",
      "iteration 900 / 1000: loss 2.171243\n",
      "iteration 0 / 1000: loss 1312.291766\n",
      "iteration 100 / 1000: loss 3.037156\n",
      "iteration 200 / 1000: loss 2.104268\n",
      "iteration 300 / 1000: loss 2.176196\n",
      "iteration 400 / 1000: loss 2.125448\n",
      "iteration 500 / 1000: loss 2.150483\n",
      "iteration 600 / 1000: loss 2.134476\n",
      "iteration 700 / 1000: loss 2.115175\n",
      "iteration 800 / 1000: loss 2.140369\n",
      "iteration 900 / 1000: loss 2.158490\n",
      "iteration 0 / 1000: loss 1228.112016\n",
      "iteration 100 / 1000: loss 3.424631\n",
      "iteration 200 / 1000: loss 2.067400\n",
      "iteration 300 / 1000: loss 2.127218\n",
      "iteration 400 / 1000: loss 2.142291\n",
      "iteration 500 / 1000: loss 2.161258\n",
      "iteration 600 / 1000: loss 2.126797\n",
      "iteration 700 / 1000: loss 2.108058\n",
      "iteration 800 / 1000: loss 2.070572\n",
      "iteration 900 / 1000: loss 2.114827\n",
      "iteration 0 / 1000: loss 1165.565209\n",
      "iteration 100 / 1000: loss 4.004001\n",
      "iteration 200 / 1000: loss 2.194404\n",
      "iteration 300 / 1000: loss 2.115432\n",
      "iteration 400 / 1000: loss 2.129993\n",
      "iteration 500 / 1000: loss 2.084200\n",
      "iteration 600 / 1000: loss 2.082799\n",
      "iteration 700 / 1000: loss 2.125387\n",
      "iteration 800 / 1000: loss 2.127499\n",
      "iteration 900 / 1000: loss 2.133766\n",
      "iteration 0 / 1000: loss 1088.078452\n",
      "iteration 100 / 1000: loss 4.860920\n",
      "iteration 200 / 1000: loss 2.114502\n",
      "iteration 300 / 1000: loss 2.070853\n",
      "iteration 400 / 1000: loss 2.133265\n",
      "iteration 500 / 1000: loss 2.147821\n",
      "iteration 600 / 1000: loss 2.154443\n",
      "iteration 700 / 1000: loss 2.166845\n",
      "iteration 800 / 1000: loss 2.097091\n",
      "iteration 900 / 1000: loss 2.087292\n",
      "iteration 0 / 1000: loss 1012.600166\n",
      "iteration 100 / 1000: loss 6.042948\n",
      "iteration 200 / 1000: loss 2.141934\n",
      "iteration 300 / 1000: loss 2.056500\n",
      "iteration 400 / 1000: loss 2.072459\n",
      "iteration 500 / 1000: loss 2.099137\n",
      "iteration 600 / 1000: loss 2.122447\n",
      "iteration 700 / 1000: loss 2.137669\n",
      "iteration 800 / 1000: loss 2.098531\n",
      "iteration 900 / 1000: loss 2.141629\n",
      "iteration 0 / 1000: loss 932.948517\n",
      "iteration 100 / 1000: loss 7.659124\n",
      "iteration 200 / 1000: loss 2.050167\n",
      "iteration 300 / 1000: loss 2.130825\n",
      "iteration 400 / 1000: loss 2.067523\n",
      "iteration 500 / 1000: loss 2.067117\n",
      "iteration 600 / 1000: loss 2.084240\n",
      "iteration 700 / 1000: loss 2.155747\n",
      "iteration 800 / 1000: loss 2.076695\n",
      "iteration 900 / 1000: loss 2.092313\n",
      "iteration 0 / 1000: loss 839.077859\n",
      "iteration 100 / 1000: loss 9.818745\n",
      "iteration 200 / 1000: loss 2.147915\n",
      "iteration 300 / 1000: loss 2.064272\n",
      "iteration 400 / 1000: loss 2.082941\n",
      "iteration 500 / 1000: loss 2.076146\n",
      "iteration 600 / 1000: loss 2.124697\n",
      "iteration 700 / 1000: loss 2.167626\n",
      "iteration 800 / 1000: loss 2.127052\n",
      "iteration 900 / 1000: loss 2.086872\n",
      "iteration 0 / 1000: loss 1548.617775\n",
      "iteration 100 / 1000: loss 2.295432\n",
      "iteration 200 / 1000: loss 2.176443\n",
      "iteration 300 / 1000: loss 2.223884\n",
      "iteration 400 / 1000: loss 2.158320\n",
      "iteration 500 / 1000: loss 2.161480\n",
      "iteration 600 / 1000: loss 2.238294\n",
      "iteration 700 / 1000: loss 2.136036\n",
      "iteration 800 / 1000: loss 2.126841\n",
      "iteration 900 / 1000: loss 2.128255\n",
      "iteration 0 / 1000: loss 1444.980830\n",
      "iteration 100 / 1000: loss 2.359795\n",
      "iteration 200 / 1000: loss 2.143274\n",
      "iteration 300 / 1000: loss 2.098225\n",
      "iteration 400 / 1000: loss 2.132205\n",
      "iteration 500 / 1000: loss 2.136099\n",
      "iteration 600 / 1000: loss 2.146831\n",
      "iteration 700 / 1000: loss 2.131059\n",
      "iteration 800 / 1000: loss 2.188813\n",
      "iteration 900 / 1000: loss 2.120222\n",
      "iteration 0 / 1000: loss 1384.267064\n",
      "iteration 100 / 1000: loss 2.485345\n",
      "iteration 200 / 1000: loss 2.149964\n",
      "iteration 300 / 1000: loss 2.143079\n",
      "iteration 400 / 1000: loss 2.184990\n",
      "iteration 500 / 1000: loss 2.176306\n",
      "iteration 600 / 1000: loss 2.133182\n",
      "iteration 700 / 1000: loss 2.160613\n",
      "iteration 800 / 1000: loss 2.123491\n",
      "iteration 900 / 1000: loss 2.097404\n",
      "iteration 0 / 1000: loss 1315.018281\n",
      "iteration 100 / 1000: loss 2.585268\n",
      "iteration 200 / 1000: loss 2.079644\n",
      "iteration 300 / 1000: loss 2.121547\n",
      "iteration 400 / 1000: loss 2.141727\n",
      "iteration 500 / 1000: loss 2.170142\n",
      "iteration 600 / 1000: loss 2.180742\n",
      "iteration 700 / 1000: loss 2.136079\n",
      "iteration 800 / 1000: loss 2.173660\n",
      "iteration 900 / 1000: loss 2.122121\n",
      "iteration 0 / 1000: loss 1235.095369\n",
      "iteration 100 / 1000: loss 2.767359\n",
      "iteration 200 / 1000: loss 2.104863\n",
      "iteration 300 / 1000: loss 2.127058\n",
      "iteration 400 / 1000: loss 2.149754\n",
      "iteration 500 / 1000: loss 2.091206\n",
      "iteration 600 / 1000: loss 2.131253\n",
      "iteration 700 / 1000: loss 2.119139\n",
      "iteration 800 / 1000: loss 2.171184\n",
      "iteration 900 / 1000: loss 2.137044\n",
      "iteration 0 / 1000: loss 1139.931569\n",
      "iteration 100 / 1000: loss 3.096438\n",
      "iteration 200 / 1000: loss 2.156361\n",
      "iteration 300 / 1000: loss 2.135300\n",
      "iteration 400 / 1000: loss 2.211083\n",
      "iteration 500 / 1000: loss 2.155752\n",
      "iteration 600 / 1000: loss 2.129042\n",
      "iteration 700 / 1000: loss 2.111049\n",
      "iteration 800 / 1000: loss 2.167618\n",
      "iteration 900 / 1000: loss 2.095384\n",
      "iteration 0 / 1000: loss 1082.359744\n",
      "iteration 100 / 1000: loss 3.654671\n",
      "iteration 200 / 1000: loss 2.096762\n",
      "iteration 300 / 1000: loss 2.109051\n",
      "iteration 400 / 1000: loss 2.132357\n",
      "iteration 500 / 1000: loss 2.146468\n",
      "iteration 600 / 1000: loss 2.131159\n",
      "iteration 700 / 1000: loss 2.143370\n",
      "iteration 800 / 1000: loss 2.107865\n",
      "iteration 900 / 1000: loss 2.128477\n",
      "iteration 0 / 1000: loss 1017.199492\n",
      "iteration 100 / 1000: loss 4.414976\n",
      "iteration 200 / 1000: loss 2.101510\n",
      "iteration 300 / 1000: loss 2.078823\n",
      "iteration 400 / 1000: loss 2.053901\n",
      "iteration 500 / 1000: loss 2.154002\n",
      "iteration 600 / 1000: loss 2.123612\n",
      "iteration 700 / 1000: loss 2.123948\n",
      "iteration 800 / 1000: loss 2.044348\n",
      "iteration 900 / 1000: loss 2.094279\n",
      "iteration 0 / 1000: loss 925.903018\n",
      "iteration 100 / 1000: loss 5.481080\n",
      "iteration 200 / 1000: loss 2.173984\n",
      "iteration 300 / 1000: loss 2.115016\n",
      "iteration 400 / 1000: loss 2.107368\n",
      "iteration 500 / 1000: loss 2.093170\n",
      "iteration 600 / 1000: loss 2.064901\n",
      "iteration 700 / 1000: loss 2.069371\n",
      "iteration 800 / 1000: loss 2.143125\n",
      "iteration 900 / 1000: loss 2.125040\n",
      "iteration 0 / 1000: loss 846.449058\n",
      "iteration 100 / 1000: loss 7.016610\n",
      "iteration 200 / 1000: loss 2.184331\n",
      "iteration 300 / 1000: loss 2.059604\n",
      "iteration 400 / 1000: loss 2.066052\n",
      "iteration 500 / 1000: loss 2.026811\n",
      "iteration 600 / 1000: loss 2.059436\n",
      "iteration 700 / 1000: loss 2.104201\n",
      "iteration 800 / 1000: loss 2.110933\n",
      "iteration 900 / 1000: loss 2.119371\n",
      "best validation accuracy achieved during cross-validation: 0.347000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "lr_lb = learning_rates[0]\n",
    "lr_ub = learning_rates[1]\n",
    "lr_step_size = 10\n",
    "lr_step = (lr_ub - lr_lb) / lr_step_size\n",
    "reg_ub = regularization_strengths[0]\n",
    "reg_lb = regularization_strengths[1]\n",
    "reg_step_size = 10\n",
    "reg_step = (reg_ub - reg_lb) / reg_step_size\n",
    "for lr in np.arange(lr_lb, lr_ub, lr_step):\n",
    "    for reg in np.arange(reg_lb, reg_ub, reg_step):\n",
    "        softmax = Softmax()\n",
    "        loss_history = softmax.train(X_train, y_train, lr, reg, num_iters=1000, batch_size=200, verbose=True)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        y_val_acc = np.mean(y_val_pred == y_val)\n",
    "        if y_val_acc > best_val:\n",
    "            best_val = y_val_acc\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.355000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvzuzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff9d9a10ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
